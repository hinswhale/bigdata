# ğŸ“– spark SQL
* [åŸºæœ¬æ¦‚å¿µ](#åŸºæœ¬æ¦‚å¿µ)
   * [DataFrame](#dataframe)
   * [DataSet](#dataset)
   * [SparkSession](#sparksession)
* [DataFrame](#dataframe-1)
   * [åŸºæœ¬ç”¨æ³•](#åŸºæœ¬ç”¨æ³•)
   * [DataFrameåˆ›å»º](#dataframeåˆ›å»º)
      * [ä»sparkæ•°æ®æºåˆ›å»º](#ä»sparkæ•°æ®æºåˆ›å»º)
      * [RDD](#rdd)
      * [Hive](#hive)
   * [SQLçš„åŸºæœ¬ä½¿ç”¨](#sqlçš„åŸºæœ¬ä½¿ç”¨)
      * [DSLè¯­æ³•çš„åŸºæœ¬ä½¿ç”¨](#dslè¯­æ³•çš„åŸºæœ¬ä½¿ç”¨)
* [DataSet](#dataset-1)
   * [åˆ›å»º](#åˆ›å»º)
* [RDD  DataFrame  DataSet è½¬åŒ–å…³ç³»](#rdd--dataframe--dataset-è½¬åŒ–å…³ç³»)
* [UDFå‡½æ•°](#udfå‡½æ•°)
   * [å¼±ç±»å‹](#å¼±ç±»å‹)
   * [å¼ºç±»å‹](#å¼ºç±»å‹)
* [æ–‡ä»¶è¯»å–](#æ–‡ä»¶è¯»å–)
   * [csv](#csv)
   * [Mysql](#mysql)
   * [Hive](#hive-1)
      * [å†…ç½®](#å†…ç½®)
      * [å¤–ç½®](#å¤–ç½®)


# åŸºæœ¬æ¦‚å¿µ
## DataFrame
```DataFrame = Schema(è¡¨ç»“æ„) + RDDï¼ˆä»£è¡¨æ•°æ®ï¼‰```
## DataSet
   æ•°æ®çš„åˆ†å¸ƒå¼é›†åˆ.Datasetæ˜¯åœ¨Spark 1.6ä¸­æ·»åŠ çš„ä¸€ä¸ªæ–°æ¥å£ï¼Œæ˜¯DataFrameä¹‹ä¸Šæ›´é«˜ä¸€çº§çš„æŠ½è±¡ã€‚
## SparkSession
 - SparkSessionä½œä¸ºDataSetå’ŒDataFrame APIçš„åˆ‡å…¥ç‚¹ï¼ŒSparkSessionå°è£…äº†SparkConfã€SparkContextå’ŒSQLContextã€‚

# DataFrame

 SparkSessionæ˜¯åˆ›å»ºDataFrameå’Œæ‰§è¡ŒSQLçš„å…¥å£

- ç¤ºä¾‹æ–‡ä»¶ï¼š
  `user.json`æ•°æ®æ ¼å¼ï¼š
    ```json
    {"username":"zhhangsan", "age":  10}
    {"username":"wanwu", "age":  20}
    {"username":"zhhangsan", "age":  30}
    ```
## åŸºæœ¬ç”¨æ³•
![img.png](img.png)
![img_1.png](img_1.png)

## DataFrameåˆ›å»º
### ä»sparkæ•°æ®æºåˆ›å»º
 `spark.read.[json,text, ....]`
   
  ä»å†…å­˜ä¸­å¯è·å–æ•°æ®ç±»å‹ï¼Œä½†æ–‡ä»¶ä¸­è¯»å–è·å–ä¸åˆ°,æ•°å­—ç”¨bigintæ¥å—ä¸äº†

### RDD
- ç¤ºä¾‹
    
  ![img_2.png](../pic/sql/RDDtoDataFrame.png)

### Hive

## SQLçš„åŸºæœ¬ä½¿ç”¨

```scala
    df.createOrReplaceTempView("user")
```

- table VS View

    `table å¯ä¿®æ”¹
     View æŸ¥è¯¢`
- æ™®é€šä¸´æ—¶è¡¨æ˜¯sessionèŒƒå›´å†…çš„ï¼Œ df.createOrReplaceGlobalTempView VS df.createOrReplaceTempView
```scala
  spark.newSession.sql("select * from global_user.user")
  spark.newSession.sql("select * from global_temp.emp")
```
### DSLè¯­æ³•çš„åŸºæœ¬ä½¿ç”¨

- æŸ¥çœ‹åˆ—æ•°æ®

![img_2.png](../pic/sql/DSL1.png)

- åˆ—æ•°æ®è¿ç®— å¦‚"age+1", æ¯ä¸ªåˆ—å‰åŠ 'æˆ–$
  
  ```df.select("age"+1).show ``` æŠ¥é”™

  ![img_3.png](../pic/sql/DSL2.png)

# DataSet

## åˆ›å»º
- DataFrame => DataSet

```shell
scala> val df = spark.read.json("/Users/liusj/spark-demo/datas/user.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]

scala> df.show
+---+---------+
|age| username|
+---+---------+
| 10|zhhangsan|
| 20|    wanwu|
| 30|zhhangsan|
+---+---------+

scala> case class emp(username:String, age:Long)
defined class emp

scala> val ds = df.as[emp]
ds: org.apache.spark.sql.Dataset[emp] = [age: bigint, username: string]

scala> ds.show
+---+---------+
|age| username|
+---+---------+
| 10|zhhangsan|
| 20|    wanwu|
| 30|zhhangsan|
+---+---------+
```

- DataSet => DataFrame

  ```ds.toDF()```

- RDD <=> DataSet

```shell
    scala> case class emp(username:String, age:Long)
    scala> val rdd = sc.makeRDD(List(emp("zhang", 30), emp("HAHAH", 10)))
    rdd: org.apache.spark.rdd.RDD[emp] = ParallelCollectionRDD[36] at makeRDD at <console>:26
    
    scala> rdd.toDS
    res16: org.apache.spark.sql.Dataset[emp] = [username: string, age: bigint]
    scala> rdd.toDS.rdd
    res17: org.apache.spark.rdd.RDD[emp] = MapPartitionsRDD[39] at rdd at <console>:26
```

# RDD  DataFrame  DataSet è½¬åŒ–å…³ç³»
![img_7.png](img_7.png)

```scala
  type DataFrame = Dataset[Row]
```
![](../pic/sql/dataset2.png)

```scala
package spark.sql

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
import org.apache.spark.{SparkConf, SparkContext}

object Basic {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local").setAppName("Bc")
    val sess = SparkSession.builder().config(sparkConf).getOrCreate()
    import sess.implicits._

    // DataFrame
    //    val df: DataFrame = sess.read.json("datas/user.json")
    //df.show()

    //DataFrame => SQL
    //    df.createOrReplaceTempView("user")
    //    sess.sql("select age, username from user").show
    //    sess.sql("select avg(age) from user").show

    //DataFrame => DSL
    //    //è½¬æ¢æ“ä½œ,éœ€å¼•å…¥è½¬æ¢è§„åˆ™
    //    df.select("age", "username").show()
    //    df.select($"age"+1).show()

    //RDD => Dataset
    //    val seq = Seq(1, 2, 3, 4)
    //    val ds: Dataset[Int] = seq.toDS()
    //    ds.show()

    //DataSet DataFrameæ˜¯ç‰¹å®šæ³›å‹DataSet
    // DataFrameæ–¹æ³•éƒ½é€‚åˆDataSet
    //    val ds = sess.read.json("datas/user.json")
    //    ds.createOrReplaceTempView("user")
    //    sess.sql("select age, username from user").show

    //RDD <=> DataFrame
    val rdd = sess.sparkContext.makeRDD(List((1, "zhangsan", 40), (2, "wangwu", 2)))
    val df = rdd.toDF("id", "name", "age")
    val rdd1: RDD[Row] = df.rdd

    //DataSet <=> DataFrame
    val ds: Dataset[User] = df.as[User] // df => ds
    ds.toDF() // ds => df


    //RDD <=> DataSet
    val ds1: Dataset[User] = rdd.map {
      case (id, name, age) => {
        User(id, name, age)
      }
    }.toDS()
    val rdd2: RDD[User] = ds1.rdd
    sess.close()

  }

  case class User(id: Int, name: String, age: Int)

}

```

# è‡ªå®šä¹‰å‡½æ•°
- UDF(User Defined Functions)æ˜¯æ™®é€šçš„ä¸ä¼šäº§ç”ŸShuffleä¸ä¼šåˆ’åˆ†æ–°çš„é˜¶æ®µçš„ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼Œ
- UDAF(User Defined Aggregator Functions)åˆ™ä¼šæ‰“ä¹±åˆ†åŒºï¼Œç”¨æˆ·è‡ªå®šä¹‰èšåˆå‡½æ•°ã€‚
## UDF
```scala
    import org.apache.spark.sql.functions  // DSLä¸­å®šä¹‰UDFéœ€è¦
    
    val rdd: RDD[User] = spark.sparkContext.makeRDD(
      List(User("Bob", 23), User("Alice", 22), User("John", 24)))
    val ds: Dataset[User] = rdd.toDS
    ds.createOrReplaceTempView("user")
    
    // SQLä¸­ä½¿ç”¨å°±éœ€è¦æ³¨å†ŒUDF
    spark.udf.register("add_name", (str: String) => { "Name: " + str })
    spark.sql("select name, add_name(name) as new_name from user").show()
    
    // ä½¿ç”¨DSLåˆ™ä¸ç”¨æ³¨å†Œï¼Œå®šä¹‰å¥½ç›´æ¥ä½¿ç”¨å³å¯
    val add_name2: UserDefinedFunction = functions.udf((str: String) => {
      "Name: " + str
    })
    ds.withColumn("name", add_name2($"name")).show()  
```
## UDAF
### å¼±ç±»å‹
- ~~UserDefinedAggregateFunction~~ å·²å¼ƒç”¨
### å¼ºç±»å‹
```text
store,user,payment
1,Bob,12.00
1,Alice,44.12
1,John,23.20
2,Davin,79.00
2,Lim,33.30
...
```

- Aggregatorï¼ˆSpark3.0ç‰ˆæœ¬ä»¥åï¼‰
  ```scala
  spark.udf.register("myudaf02", functions.udaf(new MyUDAF02))  // æ³¨å†ŒUDAFå‡½æ•°
  spark.sql(
  """
    |select store, myudaf02(payment) from record group by store
    |""".stripMargin).show(truncate = false)
  ```
  
  - UDAFç±»
  ```scala
    case class StoreSummary(var user: Int, var payment: Double)  // å¼ºç±»å‹UDAFå‡½æ•°Bufferç±»å‹
    
    class MyUDAF02 extends Aggregator[Double, StoreSummary, String] {
      
      // åˆå§‹åŒ–Bufferä¸­çš„å­—æ®µ
      override def zero: StoreSummary = {
        StoreSummary(0, 0.00)
      }
    
      // è¾“å…¥åˆ°Bufferçš„èšåˆ
      override def reduce(b: StoreSummary, a: Double): StoreSummary = {
        b.user += 1
        b.payment += a
        b
      }
    
      // åˆå¹¶Buffer
      override def merge(b1: StoreSummary, b2: StoreSummary): StoreSummary = {
        b1.user += b2.user
        b1.payment += b2.payment
        b1
      }
    
      // æœ€ç»ˆçš„è®¡ç®—ç»“æœ
      override def finish(reduction: StoreSummary): String = {
        "user: " + reduction.user + ",payment: " + reduction.payment
      }
    
      // Dataseté»˜è®¤ç¼–ç å™¨ï¼Œç”¨äºåºåˆ—åŒ–ï¼Œå›ºå®šå†™æ³•
      override def bufferEncoder: Encoder[StoreSummary] = Encoders.product
    
      override def outputEncoder: Encoder[String] = Encoders.STRING
    }
  ```
- Aggregatorï¼ˆSpark3.0ç‰ˆæœ¬ä»¥å‰ï¼‰

æ—©æœŸç‰ˆæœ¬ä¸­ä¸èƒ½åœ¨SQLä¸­ä½¿ç”¨å¼ºç±»å‹UDAFï¼Œä½†æ˜¯å¯ä»¥åœ¨DSLä¸­ä½¿ç”¨ï¼Œä»£ç ç¼–å†™å’Œè°ƒç”¨æ–¹å¼éƒ½æœ‰æ‰€ä¸åŒï¼ŒDSLæ³¨é‡çš„æ˜¯ç±»å‹ï¼Œæ‰€ä»¥åœ¨UDAFè¾“å…¥ç±»å‹è¿™é‡Œä¼ å…¥çš„åº”è¯¥æ˜¯DataSetæ¯ä¸€è¡Œçš„ç±»å‹ï¼Œè€Œä¸æ˜¯å›ºå®šå­—æ®µçš„æŸä¸ªç±»å‹ã€‚
```scala
    // æ ·ä¾‹ç±»ï¼Œå†™åœ¨mainæ–¹æ³•å¤–
    case class Record(store: Int, name: String, payment: Double)
    
    class MyUDAF03 extends Aggregator[Record, StoreSummary, String] {
    override def zero: StoreSummary = {
      StoreSummary(0, 0.00)
    }

    override def reduce(b: StoreSummary, a: Record): StoreSummary = {
      b.user += 1
      b.payment += a.payment
      b
    }

    override def merge(b1: StoreSummary, b2: StoreSummary): StoreSummary = {
      b1.user += b2.user
      b1.payment += b2.payment
      b1
    }

    override def finish(reduction: StoreSummary): String = {
      "user: " + reduction.user + ",payment: " + reduction.payment
    }

    override def bufferEncoder: Encoder[StoreSummary] = Encoders.product

    override def outputEncoder: Encoder[String] = Encoders.STRING
  }

```
# æ–‡ä»¶è¯»å–

![img_4.png](img_4.png)

```scala
scala>  sess.read.load
scala> sess.read.json("filename")
scala> spark.sql("select * from json.`datas/user.json`").show
+---+---------+
|age| username|
+---+---------+
| 10|zhhangsan|
| 20|    wanwu|
| 30|zhhangsan|
+---+---------+
scala>  df.write.save("filename")
scala>  df.write.save("filename") //æŠ¥é”™ã€‚å·²å­˜åœ¨
scala> df.write.format("json").mode("ignore").save("out") //ä¸æŠ¥é”™
```

## csv
```scala
val dataFrame: DataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("encoding", "gbk2312")
      .load(path)
```

## Mysql

![img_5.png](img_5.png)

![img_6.png](img_6.png)

## Hive
### å†…ç½®
### å¤–ç½®
- è¿æ¥æ–¹æ³•

![img_7.png](../pic/sql/Hive.png)
