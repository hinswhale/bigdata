# ğŸ“– spark SQL
* [åŸºæœ¬æ¦‚å¿µ](#åŸºæœ¬æ¦‚å¿µ)
   * [DataFrame](#dataframe)
   * [DataSet](#dataset)
   * [SparkSession](#sparksession)
* [DataFrame](#dataframe-1)
   * [åŸºæœ¬ç”¨æ³•](#åŸºæœ¬ç”¨æ³•)
   * [DataFrameåˆ›å»º](#dataframeåˆ›å»º)
      * [ä»sparkæ•°æ®æºåˆ›å»º](#ä»sparkæ•°æ®æºåˆ›å»º)
      * [RDD](#rdd)
      * [Hive](#hive)
   * [SQLçš„åŸºæœ¬ä½¿ç”¨](#sqlçš„åŸºæœ¬ä½¿ç”¨)
      * [DSLè¯­æ³•çš„åŸºæœ¬ä½¿ç”¨](#dslè¯­æ³•çš„åŸºæœ¬ä½¿ç”¨)
* [DataSet](#dataset-1)
   * [åˆ›å»º](#åˆ›å»º)
* [RDD  DataFrame  DataSet è½¬åŒ–å…³ç³»](#rdd--dataframe--dataset-è½¬åŒ–å…³ç³»)
* [UDFå‡½æ•°](#udfå‡½æ•°)
   * [å¼±ç±»å‹](#å¼±ç±»å‹)
   * [å¼ºç±»å‹](#å¼ºç±»å‹)
* [æ–‡ä»¶è¯»å–](#æ–‡ä»¶è¯»å–)
   * [csv](#csv)
   * [Mysql](#mysql)
   * [Hive](#hive-1)
      * [å†…ç½®](#å†…ç½®)
      * [å¤–ç½®](#å¤–ç½®)


# åŸºæœ¬æ¦‚å¿µ
## DataFrame
```DataFrame = Schema(è¡¨ç»“æ„) + RDDï¼ˆä»£è¡¨æ•°æ®ï¼‰```
## DataSet
   æ•°æ®çš„åˆ†å¸ƒå¼é›†åˆ.Datasetæ˜¯åœ¨Spark 1.6ä¸­æ·»åŠ çš„ä¸€ä¸ªæ–°æ¥å£ï¼Œæ˜¯DataFrameä¹‹ä¸Šæ›´é«˜ä¸€çº§çš„æŠ½è±¡ã€‚
## SparkSession
 - SparkSessionä½œä¸ºDataSetå’ŒDataFrame APIçš„åˆ‡å…¥ç‚¹ï¼ŒSparkSessionå°è£…äº†SparkConfã€SparkContextå’ŒSQLContextã€‚

# DataFrame

 SparkSessionæ˜¯åˆ›å»ºDataFrameå’Œæ‰§è¡ŒSQLçš„å…¥å£

- ç¤ºä¾‹æ–‡ä»¶ï¼š
  `user.json`æ•°æ®æ ¼å¼ï¼š
    ```json
    {"username":"zhhangsan", "age":  10}
    {"username":"wanwu", "age":  20}
    {"username":"zhhangsan", "age":  30}
    ```
## åŸºæœ¬ç”¨æ³•
![img.png](img.png)
![img_1.png](img_1.png)

## DataFrameåˆ›å»º
### ä»sparkæ•°æ®æºåˆ›å»º
 `spark.read.[json,text, ....]`
   
  ä»å†…å­˜ä¸­å¯è·å–æ•°æ®ç±»å‹ï¼Œä½†æ–‡ä»¶ä¸­è¯»å–è·å–ä¸åˆ°,æ•°å­—ç”¨bigintæ¥å—ä¸äº†

### RDD
- RDDä¸DataFrameäº’ç›¸è½¬åŒ–
    
    ![img_2.png](../pic/sql/RDDandFrame.png)
- ç¤ºä¾‹
    ![img_2.png](../pic/sql/RDDtoDataFrame.png)

### Hive

## SQLçš„åŸºæœ¬ä½¿ç”¨

```scala
    df.createOrReplaceTempView("user")
```

- table VS View

    `table å¯ä¿®æ”¹
     View æŸ¥è¯¢`
- æ™®é€šä¸´æ—¶è¡¨æ˜¯sessionèŒƒå›´å†…çš„ï¼Œ df.createOrReplaceGlobalTempView VS df.createOrReplaceTempView
```scala
  spark.newSession.sql("select * from global_user.user")
  spark.newSession.sql("select * from global_temp.emp")
```
### DSLè¯­æ³•çš„åŸºæœ¬ä½¿ç”¨

- æŸ¥çœ‹åˆ—æ•°æ®

![img_2.png](../pic/sql/DSL1.png)

- åˆ—æ•°æ®è¿ç®— å¦‚"age+1", æ¯ä¸ªåˆ—å‰åŠ 'æˆ–$
  
  ```df.select("age"+1).show ``` æŠ¥é”™

  ![img_3.png](../pic/sql/DSL2.png)

# DataSet

## åˆ›å»º
- DataFrame => DataSet
```shell
scala> val df = spark.read.json("/Users/liusj/spark-demo/datas/user.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]

scala> df.show
+---+---------+
|age| username|
+---+---------+
| 10|zhhangsan|
| 20|    wanwu|
| 30|zhhangsan|
+---+---------+

scala> case class emp(username:String, age:Long)
defined class emp

scala> val ds = df.as[emp]
ds: org.apache.spark.sql.Dataset[emp] = [age: bigint, username: string]

scala> ds.show
+---+---------+
|age| username|
+---+---------+
| 10|zhhangsan|
| 20|    wanwu|
| 30|zhhangsan|
+---+---------+
```
- DataSet => DataFrame

  ```ds.toDF()```

- RDD <=> DataSet

```shell
scala> case class emp(username:String, age:Long)
scala> val rdd = sc.makeRDD(List(emp("zhang", 30), emp("HAHAH", 10)))
rdd: org.apache.spark.rdd.RDD[emp] = ParallelCollectionRDD[36] at makeRDD at <console>:26

scala> rdd.toDS
res16: org.apache.spark.sql.Dataset[emp] = [username: string, age: bigint]
scala> rdd.toDS.rdd
res17: org.apache.spark.rdd.RDD[emp] = MapPartitionsRDD[39] at rdd at <console>:26
```

# RDD  DataFrame  DataSet è½¬åŒ–å…³ç³»
![img_7.png](img_7.png)

```scala
  type DataFrame = Dataset[Row]
```

![img_2.png](../pic/sql/dataset-rdd-dataframe.png)

```scala
package spark.sql

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
import org.apache.spark.{SparkConf, SparkContext}

object Basic {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local").setAppName("Bc")
    val sess = SparkSession.builder().config(sparkConf).getOrCreate()
    import sess.implicits._

    // DataFrame
    //    val df: DataFrame = sess.read.json("datas/user.json")
    //df.show()

    //DataFrame => SQL
    //    df.createOrReplaceTempView("user")
    //    sess.sql("select age, username from user").show
    //    sess.sql("select avg(age) from user").show

    //DataFrame => DSL
    //    //è½¬æ¢æ“ä½œ,éœ€å¼•å…¥è½¬æ¢è§„åˆ™
    //    df.select("age", "username").show()
    //    df.select($"age"+1).show()


    //    val seq = Seq(1, 2, 3, 4)
    //    val ds: Dataset[Int] = seq.toDS()
    //    ds.show()

    //DataSet DataFrameæ˜¯ç‰¹å®šæ³›å‹DataSet
    // DataFrameæ–¹æ³•éƒ½é€‚åˆDataSet
    //    val ds = sess.read.json("datas/user.json")
    //    ds.createOrReplaceTempView("user")
    //    sess.sql("select age, username from user").show

    //RDD <=> DataFrame
    val rdd = sess.sparkContext.makeRDD(List((1, "zhangsan", 40), (2, "wangwu", 2)))
    val df = rdd.toDF("id", "name", "age")
    val rdd1: RDD[Row] = df.rdd

    //DataSet <=> DataFrame
    val ds: Dataset[User] = df.as[User] // df => ds
    ds.toDF() // ds => df


    //RDD <=> DataSet
    val ds1: Dataset[User] = rdd.map {
      case (id, name, age) => {
        User(id, name, age)
      }
    }.toDS()
    val rdd2: RDD[User] = ds1.rdd
    sess.close()

  }

  case class User(id: Int, name: String, age: Int)

}

```

# UDFå‡½æ•°
## å¼±ç±»å‹
## å¼ºç±»å‹

# æ–‡ä»¶è¯»å–

![img_4.png](img_4.png)

```scala
scala>  sess.read.load
scala> sess.read.json("filename")
scala> spark.sql("select * from json.`datas/user.json`").show
+---+---------+
|age| username|
+---+---------+
| 10|zhhangsan|
| 20|    wanwu|
| 30|zhhangsan|
+---+---------+
scala>  df.write.save("filename")
scala>  df.write.save("filename") //æŠ¥é”™ã€‚å·²å­˜åœ¨
scala> df.write.format("json").mode("ignore").save("out") //ä¸æŠ¥é”™
```

## csv
```scala
val dataFrame: DataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("encoding", "gbk2312")
      .load(path)
```

## Mysql

![img_5.png](img_5.png)

![img_6.png](img_6.png)

## Hive
### å†…ç½®
### å¤–ç½®
- è¿æ¥æ–¹æ³•

![img_7.png](../pic/sql/Hive.png)
