# RDD

> ğŸ“Œ **å…³é”®è¯ï¼š** `RDD`ã€`stage`ã€`Transformations& Actions`ã€`å®½ä¾èµ–&çª„ä¾èµ–`ã€`åˆ†åŒº`


<!-- TOC depthFrom:2 depthTo:3 -->

- [1. rddæ˜¯ä»€ä¹ˆ](#1-rddæ˜¯ä»€ä¹ˆ)
- [2. åˆ›å»ºæ–¹å¼](#2-åˆ›å»ºæ–¹å¼)
    - [2.1. é€šè¿‡è¯»å–æ–‡ä»¶ç”Ÿæˆçš„](#31-é€šè¿‡è¯»å–æ–‡ä»¶ç”Ÿæˆçš„)
    - [2.2. é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDD](#32-é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDD)
- [3. rddå¹¶è¡Œåº¦ä¸åˆ†åŒº](#3-rddå¹¶è¡Œåº¦ä¸åˆ†åŒº)
- [4. å¸¸ç”¨RDDç®—å­](#4-ç¯å¢ƒå˜é‡)
    - [4.1. Transformation](#41-Transformation)
    - [4.2. Action](#42-Action)
- [5. RDDä¾èµ–å…³ç³»](#5-RDDä¾èµ–å…³ç³»)
    - [5.1. å®½ä¾èµ–](#51-å®½ä¾èµ–)
    - [5.2. çª„ä¾èµ–](#52-çª„ä¾èµ–)
    - [5.3. æ€»ç»“](#53-æ€»ç»“)
- [6. DAGçš„ç”Ÿæˆå’Œåˆ’åˆ†Stage](#6-DAGçš„ç”Ÿæˆå’Œåˆ’åˆ†Stage)
    - [6.1. stageåˆ’åˆ†](#61-stageåˆ’åˆ†)
    - [6.3. DAG/job/Action/åˆ†åŒº/å…³ç³»](#62-DAG/job/Action/åˆ†åŒº/å…³ç³»)
- [7. æŒä¹…åŒ–/ç¼“å­˜](#7-æŒä¹…åŒ–/ç¼“å­˜)
    - [7.1. cache](#71-cache)
    - [7.2. checkpoint](#71-checkpoint)
- [8. æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)

<!-- /TOC -->

# 1. rddæ˜¯ä»€ä¹ˆ

RDD(Resilient Distributed Dataset) å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†

### å®šä¹‰

- æ•°æ®é›†
    - å­˜å‚¨æ•°æ®çš„è®¡ç®—é€»è¾‘
- åˆ†å¸ƒå¼
    - æ•°æ®æ¥æºï¼šä»ä¸åŒç½‘ç»œç»“ç‚¹è¯»å–æ•°æ®
    - è®¡ç®—ï¼šæ‹†åˆ†æˆä¸åŒä»»åŠ¡ï¼Œå‘é€ç»™ä¸åŒçš„excute
    - æ•°æ®å­˜å‚¨
- å¼¹æ€§
    - ä¾èµ–å…³ç³»
        - ç»´æŠ¤è¡€ç¼˜å…³ç³»
    - è®¡ç®—
        - è®¡ç®—åŸºäºå†…å­˜ï¼Œæ€§èƒ½é«˜ï¼Œå¯ä¸ç£ç›˜çµæ´»åˆ‡æ¢
    - åˆ†åŒº
        - å¯é€šè¿‡æŒ‡å®šç®—å­æ”¹å˜åˆ†åŒºæ•°é‡ï¼Œå¹¶è¡Œè®¡ç®—
        - æ•°æ®ä¸åŒï¼Œè®¡ç®—é€»è¾‘ç›¸åŒ
    - å®¹é”™
        - é‡è¯•å¤„ç†

### ç‰¹æ€§ï¼š

* åªè¯» - åªèƒ½é€šè¿‡ç®—å­åˆ›å»ºæ–°çš„RDDï¼ŒåŸRDDæœªå—å½±å“ã€ç®—å­ã€‘
* åˆ†åŒº - æ¯ä¸ª RDD è¢«åˆ‡åˆ†æˆå¤šä¸ªåˆ†åŒº(partition), æ¯ä¸ªåˆ†åŒºå¯èƒ½ä¼šåœ¨é›†ç¾¤ä¸­ä¸åŒçš„èŠ‚ç‚¹ä¸Šè¿›è¡Œè®¡ç®—
* ä¾èµ–å…³ç³»å¯å¹¶è¡Œ - DAGï¼Œåˆ’åˆ†å¹¶è¡Œ
* ç¼“å­˜&æŒä¹…åŒ–

# 2. åˆ›å»ºæ–¹å¼

## 2.1. é€šè¿‡è¯»å–æ–‡ä»¶ç”Ÿæˆçš„

- ä»¥è¡Œä¸ºå•ä½ï¼Œè¯»å–æ•°æ®éƒ½æ˜¯å­—ç¬¦ä¸²

```scala
val rdd1 = sc.textFile("hdfs://node1:8020/wordcount/input/words.txt")
```

- ä»¥æ–‡ä»¶ä¸ºå•ä½

```scala
val rdd1 = sc.whoTextFiles("hdfs://node1:8020/wordcount/input/words.txt")
```

## 2.2. é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDD

```scala
num_rdd = sc.parallelize([1,2,3]) //parallelize å¹¶è¡Œ
```

æˆ–

```scala
val rdd = sc.makeRDD(List(1, 2, 3, 4), 2) // 2ä¸ºåˆ†åŒºï¼Œä¸å†™ä¼šæœ‰é»˜è®¤å€¼
```

makeRDDæ–¹æ³•åº•å±‚è°ƒç”¨äº†parallelizeæ–¹æ³•

## 2.3. å…¶ä»–æ–¹å¼

# 3 rddå¹¶è¡Œåº¦ä¸åˆ†åŒº

## åˆ†åŒºç®—æ³•

![img.png](../../pic/åˆ†åŒº.png)

- åç§»é‡ ä¾‹å­ï¼š æœ€å°åˆ†åŒºï¼š3 æ–‡ä»¶å†…å®¹ï¼š
  ```text
    1
    2
    3
   ```
  ç»“æœï¼š æ–‡ä»¶1ï¼š 1 2 æ–‡ä»¶2ï¼š3 æ–‡ä»¶3ï¼š ç©º

# 4. å¸¸ç”¨RDDç®—å­

**`æƒ°æ€§æ±‚å€¼`**

- Transformations ï¼šä»ä¸€ä¸ªå·²çŸ¥çš„ RDD ä¸­åˆ›å»ºå‡ºæ¥ä¸€ä¸ªæ–°çš„ RDD
- Actionsï¼š åœ¨æ•°æ®é›†ä¸Šè®¡ç®—ç»“æŸä¹‹å, ç»™é©±åŠ¨ç¨‹åºè¿”å›ä¸€ä¸ªå€¼

åœ¨ Spark ä¸­å‡ ä¹æ‰€æœ‰çš„transformationæ“ä½œéƒ½æ˜¯`æ‡’æ‰§è¡Œ`çš„(lazy).

åªæœ‰é‡åˆ°actionï¼Œæ‰ä¼šæ‰§è¡Œ RDD çš„è®¡ç®—(å³å»¶è¿Ÿè®¡ï¼‰,åªæœ‰é‡åˆ°actionï¼Œæ‰ä¼šæ‰§è¡Œ RDD çš„è®¡ç®—(å³å»¶è¿Ÿè®¡ï¼‰

## 4.1. Transformation

å°†æ—§çš„RDDåŒ…è£…æˆæ–°çš„RDD

### å•Valueç±»å‹

1. map

* ä¸€ä¸ªåˆ†åŒº æ•°æ®é¡ºåºæ‰§è¡Œ
* ä¸åŒåˆ†åŒºé—´ æ— åº

```
scala > val rdd1 = sc.parallelize(1 to 10)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
// å¾—åˆ°ä¸€ä¸ªæ–°çš„ RDD, ä½†æ˜¯è¿™ä¸ª RDD ä¸­çš„å…ƒç´ å¹¶ä¸æ˜¯ç«‹å³è®¡ç®—å‡ºæ¥çš„
scala> val rdd2 = rdd1.map(_ * 2, numslice=2) //numslice åˆ†åŒº
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at
<console>:26

// å¼€å§‹è®¡ç®— rdd2 ä¸­çš„å…ƒç´ , å¹¶æŠŠè®¡ç®—åçš„ç»“æœä¼ é€’ç»™é©±åŠ¨ç¨‹åº
scala> rdd2.collect
res0: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)
  ```  

2. mapPartitions è€ƒè™‘åˆ†åŒº

* è¯­æ³•

  ```mapPartitions(func) Iterator<T> => Iterator<U>```

* å¯åº”ç”¨åœºæ™¯ a. æ¯ä¸ªåˆ†åŒºæœ€å¤§å€¼æˆ–å¯¹åˆ†åŒºæ•°æ®åšæ‰¹å¤„ç†

* ğŸ `map vs mapPartitions`
    * mapPartitions æ€§èƒ½æ›´é«˜ï¼Œæ¯ä¸ªåˆ†åŒºä¸€æ¬¡æ‹¿åˆ°æ‰€æœ‰æ•°æ®

      e.g. å‡è®¾æœ‰Nä¸ªå…ƒç´ ï¼Œæœ‰Mä¸ªåˆ†åŒºï¼Œé‚£ä¹ˆmapçš„å‡½æ•°çš„å°†è¢«è°ƒç”¨Næ¬¡, è€ŒmapPartitionsè¢«è°ƒç”¨Mæ¬¡,ä¸€ä¸ªå‡½æ•°ä¸€æ¬¡å¤„ç†æ‰€æœ‰åˆ†åŒºã€‚
    * mapPartitions å†…å­˜æœ‰é™æ—¶ä¸æ¨è å¤„ç†å®Œçš„æ•°æ®ä¸ä¼šè¢«é‡Šæ”¾ï¼Œå­˜åœ¨å¯¹è±¡å¼•ç”¨ï¼Œæ•°æ®é‡è¾ƒå¤§çš„æ—¶å€™ï¼Œå®¹æ˜“å†…å­˜æº¢å‡ºï¼Œæ­¤æ—¶åº”è€ƒè™‘map
    * map è½¬æ¢åæ•°é‡ä¸å˜ï¼ŒmapPartitionså¯ä»¥æ”¹å˜

```scala
package spark.core.rdd.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object mapPartitions {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)
//    val mpRDD: RDD[Int] = rdd.mapPartitions(
//      iter => {
//        println(">>>>")
//        iter.map(_ * 2)
//      }
//    )

    // æœ€å¤§å€¼
    val mpRDD: RDD[Int] = rdd.mapPartitions(
      iter => {
        List(iter.max).iterator
      }
    )

    mpRDD.collect().foreach(println)

    sc.stop()
  }
}

```

3. mapPartitionsWithIndex ç´¢å¼•å·

* è¯­æ³•

  ``` mapPartitionsWithIndex(func) (Int, Iterator<T>) => Iterator<U>```
* åŠŸèƒ½
    * å¤šæä¾›ä¸€ä¸ªIntå€¼æ¥è¡¨ç¤ºåˆ†åŒºçš„ç´¢å¼•
    * åˆ†åŒºæ•°çš„ç¡®å®š, å’Œå¯¹æ•°ç»„ä¸­çš„å…ƒç´ å¦‚ä½•è¿›è¡Œåˆ†åŒº
* ç¤ºä¾‹ä»£ç  å–æŸä¸ªåˆ†åŒºæ•°æ®

```scala
package spark.core.rdd.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object mapPartitionsWithIndex {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)
    val mpiRDD: RDD[Int] = rdd.mapPartitionsWithIndex(
      (index, iter) => {
        if (index == 1) {
          iter
        } else {
          Nil.iterator
        }
      }
    )

    mpiRDD.collect().foreach(println)

    sc.stop()

  }
}
```
     
4. flatMap æ‰å¹³åŒ–
   * åŠŸèƒ½ æ‰å¹³åŒ– 
       * å¦‚[[1, 2] [3, 4]] = > [1, 2, 3, 4]
       * æ‹†åˆ†å•è¯ "Hello Scala", "Hello Spark" => Hello  Scala Hello Spark
  
   * ä»£ç 

```scala
/**
[1, 2] [3, 4] = > [1, 2, 3, 4]
/**
package spark.core.rdd.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object flatMap {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    val rdd: RDD[List[Int]] = sc.makeRDD(List(List(1, 2), List(3, 4)), 2)
    val flatrdd: RDD[Int] = rdd.flatMap(
      List => {List}
    )

    flatrdd.collect().foreach(println)

    sc.stop()

  }
}
  ```

* ï½æ¨¡å¼åŒ¹é…ï½
```scala
val rdd = sc.makeRDD(List(List(1, 2), 3, List(4, 5)))

val flatMapRDD = rdd.flatMap(
  data => {
    data match {
      case list: List[_] => list
      case dat => List(dat)
    }
  }
)
```
5. glom å°†æ¯ä¸ªåˆ†åŒºå½¢æˆä¸€ä¸ªæ•°ç»„
   * ç”¨æ³•
     ```RDD[Array[T]] ```
   * åŠŸèƒ½ 
     * å°†æ¯ä¸ªåˆ†åŒºå½¢æˆä¸€ä¸ªæ•°ç»„
     * åˆ†åŒºä¸ªæ•°ä¸å˜
   * ä»£ç 
   
   æ‰“å°å„åˆ†åŒºæ•°æ®

   ```scala
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)
    val glomRDD: RDD[Array[Int]] = rdd.glom(
    )

    glomRDD.foreach(arr => println(arr.mkString(",")))
   /*
    1,2
    3,4
   */
   ```
   æ±‚å„åˆ†åŒºæœ€å¤§å€¼ä¹‹å’Œ

    ```scala
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)
    val glomRDD: RDD[Array[Int]] = rdd.glom()
    val maxRDD: RDD[Int] = glomRDD.map(array => {
      array.max
    })
    println(maxRDD.collect().sum)
    // ç»“æœï¼š6
    ```
  
6. groupBy
   * ç”¨æ³•
     ```groupBy(func) RDD[(K, Iterable[T])```
   * åŠŸèƒ½ 
     * æŒ‰ç…§funcçš„è¿”å›å€¼åšä¸ºkeyè¿›è¡Œåˆ†ç»„
     * shuffle
     
   * ä»£ç 
   
    ```scala
    val rdd = sc.makeRDD(List(1, 2, 3, 4))
    val groupRDD: RDD[(Int, Iterable[Int])] = rdd.groupBy((_%2)
    groupRDD.collect().foreach(println)
    ```

7. filter
   * ç”¨æ³•
     `````
   * åŠŸèƒ½ 
     * è¿‡æ»¤
     * äº§ç”Ÿæ•°æ®å€¾æ–œ åˆ†åŒºå„åŒºæ•°æ®å·®åˆ«è¾ƒå¤§

8. sample éšæœºé‡‡æ ·
   * ç”¨æ³•
    sampleç®—å­éœ€è¦ä¼ é€’ä¸‰ä¸ªå‚æ•°
     * ç¬¬ä¸€ä¸ªå‚æ•°  æŠ½å–æ•°æ®åæ˜¯å¦å°†æ•°æ®è¿”å› 
       * trueï¼ˆè¿”å›ï¼šï¼ˆæ³Šæ¾ç®—æ³•ï¼‰ï¼‰ falseï¼ˆä¸è¿”å›ï¼šï¼ˆä¼¯åŠªåˆ©ç®—æ³•ï¼‰ï¼‰
     * ç¬¬äºŒä¸ªå‚æ•° æ¯ä¸ªæ•°æ®å‡ºç°æ¦‚ç‡
        * å¦‚æœæŠ½å–ä¸æ”¾å›çš„åœºåˆï¼šæ•°æ®æºä¸­æ¯æ¡æ•°æ®è¢«æŠ½å–çš„æ¦‚ç‡,åŸºå‡†å€¼çš„æ¦‚å¿µ
        * å¦‚æœæŠ½å–æ”¾å›çš„åœºåˆï¼šè¡¨ç¤ºæ•°æ®æºä¸­çš„æ¯æ¡æ•°æ®è¢«æŠ½å–çš„å¯èƒ½æ¬¡æ•°
     * ç¬¬ä¸‰ä¸ªå‚æ•° éšæœºç®—æ³•ç§å­ å¦‚æœä¸ä¼ ï¼Œåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
   * åŠŸèƒ½ 
     * æ•°æ®å€¾æ–œæ—¶åº”ç”¨

  * ä»£ç 

  ```scala
  import org.apache.spark.rdd.RDD
  import org.apache.spark.{SparkConf, SparkContext}
  
  object Spark08_RDD_Operator_Transform {
    def main(args: Array[String]): Unit = {
      val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
      val sc = new SparkContext(sparkConf)
  
      // TODO ç®—å­ â€”â€” sample
      val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
  
      /*val sampleRDD: RDD[Int] = rdd.sample(
        false,
        0.4
        //1
      )*/
  
      val sampleRDD: RDD[Int] = rdd.sample(
        true,
        2
        //1
      )
  
      println(sampleRDD.collect().mkString(","))
      sc.stop()
    }
  }

```

10. distinct
```scala
1import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Spark09_RDD_Operator_Transform {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    // TODO ç®—å­ â€”â€” distinct
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4, 1, 2, 3, 4))

    //map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1)
    //(1,null),(2,null),(3,null),(4,null),(1,null),(2,null),(3,null),(4,null) map
    //(1,null),(1,null)... reduceByKey
    //(null,null) => null...
    //(1,null) => 1...  map
    val distinctRDD: RDD[Int] = rdd.distinct()

    println(distinctRDD.collect().mkString(","))
    sc.stop()
  }
```
11. coalesce ç¼©å‡/æ‰©å¤§åˆ†åŒº
   * åŠŸèƒ½
     - ç¼©å‡åˆ†åŒºæ•°ï¼Œç¬¬äºŒä¸ªå‚æ•°shuffle
       - é»˜è®¤å‚æ•°: æ•°æ®æ²¡æœ‰è¢«æ‰“ä¹±ï¼Œå¯èƒ½å¯¼è‡´`æ•°æ®å€¾æ–œ`
       - å¦‚æœæƒ³è®©æ•°æ®å‡è¡¡ï¼Œå¯ä»¥ä½¿ç”¨shuffleè¿›è¡Œå¤„ç†
    - æ‰©å¤§åˆ†åŒºï¼Œå¿…é¡»shuffleï¼Œå¦åˆ™è‹¥ä¸èƒ½æ‰“ä¹±æ•°æ®ï¼Œç›¸å½“äºæ²¡æœ‰èµ·ä½œä¸š
   * ä»£ç 
```scala
package spark.core.rdd.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object coalesce {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    // TODO ç®—å­ â€”â€” coalesce
    val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4, 5, 6), 3)

    //coalesceæ–¹æ³•é»˜è®¤æƒ…å†µä¸‹ä¸ä¼šå°†åˆ†åŒºçš„æ•°æ®æ‰“ä¹±é‡æ–°ç»„åˆï¼ˆshuffleï¼‰
    //è¿™ç§æƒ…å†µä¸‹çš„ç¼©å‡åˆ†åŒºå¯èƒ½ä¼šé€ æˆæ•°æ®ä¸å‡è¡¡ï¼Œå‡ºç°æ•°æ®çš„å€¾æ–œ
    //val newRDD: RDD[Int] = rdd.coalesce(2)

    //å¦‚æœæƒ³è®©æ•°æ®å‡è¡¡ï¼Œå¯ä»¥ä½¿ç”¨shuffleè¿›è¡Œå¤„ç†
    val newRDD: RDD[Int] = rdd.coalesce(2, true)
    newRDD.saveAsTextFile("output")

    sc.stop()
  }
}
   ```

13. repartition 
    * ç”¨æ³•
      * æ‰©å¤§åˆ†åŒºï¼Œåº•å±‚æ˜¯coalesceï¼Œå‚æ•°ï¼šshuffle
    1. sortBy æ ¹æ®æŒ‡å®šè§„åˆ™æ’åº
       * ä»£ç 
        ```scala
       package spark.core.rdd.transform
    
    
       import org.apache.spark.rdd.RDD
       import org.apache.spark.{SparkConf, SparkContext}
    
       object sortBy {
         def main(args: Array[String]): Unit = {
           val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
           val sc = new SparkContext(sparkConf)
    
           // TODO ç®—å­ â€”â€” coalesce
           val rdd: RDD[Int] = sc.makeRDD(List(1, 4, 2, 5, 3), 2)
    
           val mapRDD: RDD[Int] = rdd.sortBy(num => num)
           mapRDD.saveAsTextFile("sort_output")
    
           sc.stop()
         }
       }
       /*
       ä¸¤ä¸ªåˆ†åŒºï¼š
       1, 2, 3
       4, 5, 6
       */
       ```

- åŒ Value ç±»å‹
1. intersection 
2. union 
3. subtract 
4. zip ä¸€ä¸€å¯¹åº” åˆ†åŒºæ•°é‡ç›¸åŒï¼Œåˆ†åŒºå†…æ•°æ®ç±»å‹ç›¸åŒ
    * List(1, 2, 3, 4)ï¼ŒList(3, 4, 5, 6)=> (1,3),(2,4),(3,5),(4,6)

äº¤é›†ã€å¹¶é›†å’Œå·®é›†è¦æ±‚ä¸¤ä¸ªæ•°æ®æºæ•°æ®ç±»å‹è¦ä¿æŒä¸€è‡´

-  Key-Value ç±»å‹
  1. partitionBy
  2. groupByKey
  3. reduceByKey
  4. aggregateByKey
  5. foldByKey
  6. combineByKey
  7. sortByKey
  8. mapValues
  9. join
  10. cogroup

## 4.2. Action

è§¦å‘ä»»åŠ¡è°ƒåº¦å’Œä½œä¸šçš„æ‰§è¡Œ
  1. reduce
  2. collect
  3. first
  4. take
  5. takeOrdered
  6. aggregate åˆ†åŒºé—´åˆå§‹å€¼
  7. foreach
  8. countByKey
  9. save

# 5. RDDä¾èµ–å…³ç³»

æ˜¯å¦shuffle
![img.png](../../pic/ä¾èµ–å…³ç³».png)
## 5.1. å®½ä¾èµ–
åŒ…å«Shuffleè¿‡ç¨‹ï¼Œæ— æ³•å®ç°æµæ°´çº¿æ–¹å¼å¤„ç†
- çˆ¶ RDD çš„åˆ†åŒºè¢«ä¸æ­¢ä¸€ä¸ªå­ RDD çš„åˆ†åŒºä¾èµ–
- å…·æœ‰å®½ä¾èµ–çš„ transformations åŒ…æ‹¬: sort, reduceByKey, groupByKey, join, å’Œè°ƒç”¨rePartitionå‡½æ•°çš„ä»»ä½•æ“ä½œ.

## 5.2. çª„ä¾èµ–
å¯ä»¥å®ç°æµæ°´çº¿ä¼˜åŒ–
- çˆ¶ RDD ä¸­çš„æ¯ä¸ªåˆ†åŒºæœ€å¤šåªæœ‰ä¸€ä¸ªå­åˆ†åŒº, å½¢è±¡çš„æ¯”å–»ä¸ºç‹¬ç”Ÿå­å¥³
- å¯ä»¥åœ¨ä»»ä½•çš„çš„ä¸€ä¸ªåˆ†åŒºä¸Šå•ç‹¬æ‰§è¡Œ, è€Œä¸éœ€è¦å…¶ä»–åˆ†åŒºçš„ä»»ä½•ä¿¡æ¯.

## 5.3 æ€»ç»“
`shuffle` æ“ä½œæ˜¯ spark ä¸­æœ€è€—æ—¶çš„æ“ä½œ,åº”å°½é‡é¿å…ä¸å¿…è¦çš„ `shuffle`.

# 6. DAGçš„ç”Ÿæˆå’Œåˆ’åˆ†Stage
![img.png](../../pic/stage.png)

åˆ’åˆ†stageçš„ä¾æ®å°±æ˜¯RDDä¹‹é—´çš„å®½çª„ä¾èµ–

Sparkä»»åŠ¡ä¼šæ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå½¢æˆä¸€ä¸ªDAGæœ‰å‘æ— ç¯å›¾ï¼ŒDAGä¼šæäº¤ç»™DAGSchedulerï¼Œ
DAGSchedulerä¼šæŠŠDAGåˆ’åˆ†æˆäº’ç›¸ä¾èµ–çš„å¤šä¸ªstageã€‚

æ ¸å¿ƒç®—æ³•ï¼šå›æº¯ç®—æ³• ä»åå¾€å‰å›æº¯/åå‘è§£æï¼Œé‡åˆ°çª„ä¾èµ–åŠ å…¥æœ¬Stageï¼Œé‡è§å®½ä¾èµ–è¿›è¡ŒStageåˆ‡åˆ†ã€‚


## 6.1 stageåˆ’åˆ†
- å¯¹äºçª„ä¾èµ–ï¼Œpartitionçš„è½¬æ¢å¤„ç†åœ¨Stageä¸­å®Œæˆè®¡ç®—ã€‚
- å¯¹äºå®½ä¾èµ–ï¼Œç”±äºæœ‰Shuffleçš„å­˜åœ¨ï¼Œåªèƒ½åœ¨parent RDDå¤„ç†å®Œæˆåï¼Œæ‰èƒ½å¼€å§‹æ¥ä¸‹æ¥çš„è®¡ç®—ï¼Œå› æ­¤å®½ä¾èµ–æ˜¯åˆ’åˆ†Stageçš„ä¾æ®ã€‚

## 6.2 DAG/job/Action/åˆ†åŒº/å…³ç³»
![img.png](../../pic/stageåˆ†æ.png)


* æ¦‚å¿µ
  - Application
  - job  æ‰§è¡Œä¸€ä¸ªè¡ŒåŠ¨æ“ä½œï¼Œå°±ä¼šæ‰§è¡Œsc.runJob(...)
  - stages 
  - tasks æœ€å°æ‰§è¡Œå•ä½  ä¸€ä¸ªåˆ†åŒºåˆ’ä¸€ä¸ªTask, æ¯ä¸€ä¸ª task è¡¨ç°ä¸ºä¸€ä¸ªæœ¬åœ°è®¡ç®—

* è”ç³»âš ï¸
  - - Application->Job->Stage-> Task 1å¯¹å¤š
  - æœ‰å‡ ä¸ªActionï¼Œå°±æœ‰å‡ ä¸ªDAG,ä¸€ä¸ªç¨‹åºå¯æœ‰å¤šä¸ªDAG
  - ä¸€ä¸ªDAGå¯ä»¥æœ‰å¤šä¸ªStageã€æ ¹æ®å®½ä¾èµ–/shuffleè¿›è¡Œåˆ’åˆ†ã€‘
  - åŒä¸€ä¸ªStageå¯ä»¥æœ‰å¤šä¸ªTaskå¹¶è¡Œæ‰§è¡Œ(taskæ•°=åˆ†åŒºæ•°ï¼Œå¦‚ä¸Šå›¾ï¼ŒStage1 ä¸­æœ‰ä¸‰ä¸ªåˆ†åŒºP1ã€P2ã€P3ï¼Œå¯¹åº”çš„ä¹Ÿæœ‰ä¸‰ä¸ª Task)



# 7. æŒä¹…åŒ–/ç¼“å­˜
## cache
- å°†è¯¥ RDD ç¼“å­˜èµ·æ¥ï¼Œè¯¥ RDD åªæœ‰åœ¨ç¬¬ä¸€æ¬¡è®¡ç®—çš„æ—¶å€™ä¼šæ ¹æ®è¡€ç¼˜å…³ç³»å¾—åˆ°åˆ†åŒºçš„æ•°æ®ï¼Œåœ¨åç»­å…¶ä»–åœ°æ–¹ç”¨åˆ°è¯¥ RDD çš„æ—¶å€™ï¼Œä¼šç›´æ¥ä»ç¼“å­˜å¤„å–è€Œä¸ç”¨å†æ ¹æ®è¡€ç¼˜å…³ç³»è®¡ç®—ï¼Œè¿™æ ·å°±åŠ é€ŸåæœŸçš„é‡ç”¨
## checkpoint


## 8. æ€§èƒ½ä¼˜åŒ–

## 9. å‚è€ƒèµ„æ–™
- [å°šç¡…è°·å¤§æ•°æ®Sparkæ•™ç¨‹ä»å…¥é—¨åˆ°ç²¾é€š](https://www.bilibili.com/video/BV11A411L7CK) ğŸ“š

